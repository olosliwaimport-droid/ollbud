# app/chat_agent.py
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from openai import OpenAI
from app.pricing import estimate_offer

client = OpenAI()  # u≈ºywa OPENAI_API_KEY z env

SYSTEM_PROMPT = (
    "Jeste≈õ asystentem firmy OLLBUD. "
    "Prowadzisz uprzejmƒÖ rozmowƒô o szybkiej wycenie prac remontowych/wyko≈Ñczeniowych. "
    "Dopytujesz kr√≥tko tylko o potrzebne rzeczy (typ obiektu: blok/kamienica/dom/deweloperski, powierzchnia m¬≤, zakres), "
    "a gdy masz metra≈º i standard, wywo≈Çujesz narzƒôdzie estimate_offer. "
    "M√≥w po polsku, zwiƒô≈∫le, bez kwot brutto (tylko netto + stawka VAT)."
)

# Narzƒôdzie (‚Äûfunction‚Äù) widoczne dla modelu
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "estimate_offer",
            "description": "Policz orientacyjny koszt na podstawie powierzchni i standardu.",
            "parameters": {
                "type": "object",
                "properties": {
                    "area_m2": {"type": "number", "description": "Powierzchnia w m2"},
                    "standard": {
                        "type": "string",
                        "enum": ["blok", "kamienica", "dom", "deweloperski"],
                        "description": "Typ/standard prac"
                    }
                },
                "required": ["area_m2", "standard"]
            }
        }
    }
]

class ChatTurn(BaseModel):
    role: str
    content: str

def run_chat_agent(history: List[ChatTurn]) -> Dict[str, Any]:
    """
    Przyjmuje historiƒô czatu (user/assistant), zwraca:
    - gdy model odpowie tekstem -> {"reply": "..."}
    - gdy model wezwie narzƒôdzie -> uruchamia estimate_offer() i zwraca wynik w odpowiedzi
    """
    # budujemy wiadomo≈õci (doklejamy system prompt na czele)
    messages = [{"role": "system", "content": SYSTEM_PROMPT}] + [
        {"role": t.role, "content": t.content} for t in history
    ]

    # 1. Zapytaj model z tools
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        tools=TOOLS,
        tool_choice="auto",
        temperature=0.3
    )

    choice = resp.choices[0]
    msg = choice.message

    # 2. Je≈ºeli model chce wywo≈Çaƒá narzƒôdzie
    if msg.tool_calls:
        for tool in msg.tool_calls:
            if tool.function.name == "estimate_offer":
                import json
                args = json.loads(tool.function.arguments or "{}")
                area_m2 = float(args.get("area_m2", 0))
                standard = (args.get("standard") or "blok").lower()

                # lokalne wywo≈Çanie funkcji (bez requestu HTTP)
                result = estimate_offer(area_m2, standard)

                # 3. Daj modelowi wynik narzƒôdzia, by ≈Çadnie sformu≈Çowa≈Ç odpowied≈∫
                messages += [
                    {"role": "assistant", "content": None, "tool_calls": [tool.dict()]},
                    {
                        "role": "tool",
                        "tool_call_id": tool.id,
                        "name": "estimate_offer",
                        "content": str(result)
                    },
                ]

followup = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    temperature=0.3
)

reply_text = followup.choices[0].message.content.strip()

# Dopisz informacjƒô ko≈ÑcowƒÖ o wizji lokalnej
reply_text += (
    "\n\nüìç *Dok≈Çadna wycena mo≈ºliwa jest po wizji lokalnej.* "
    "Koszt wizji lokalnej wynosi **od 400 do 1250 z≈Ç netto**, "
    "w zale≈ºno≈õci od zakresu inwestycji.\n"
    "Dziƒôkujemy za uwagƒô i do zobaczenia!"
)

return {"reply": reply_text, "raw": result}

    # 4. Zwyk≈Ça odpowied≈∫
    return {"reply": msg.content}
